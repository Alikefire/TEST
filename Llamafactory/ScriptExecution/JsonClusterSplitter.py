#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
JSONLæ–‡ä»¶èšç±»åˆ†å‰²è„šæœ¬
åŸºäºæ¨¡å‹å›å¤é•¿åº¦ä½¿ç”¨K-meansèšç±»è‡ªåŠ¨åˆ†ç»„
æ”¯æŒShareGPTå’ŒInstructionæ ¼å¼
"""

import json
import os
import argparse
import numpy as np
from pathlib import Path
from collections import defaultdict
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
from typing import Dict, List, Tuple, Any


class JsonlClusterSplitter:
    def __init__(self, input_file: str, output_dir: str, n_clusters: int = 5):
        self.input_file = input_file
        self.output_dir = Path(output_dir)
        self.n_clusters = n_clusters
        
        # æ•°æ®å­˜å‚¨
        self.records = []
        self.lengths = []
        self.cluster_labels = None
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_records': 0,
            'format_counts': {'sharegpt': 0, 'instruction': 0, 'unknown': 0},
            'length_stats': {'min': float('inf'), 'max': 0, 'mean': 0, 'std': 0},
            'cluster_info': {}
        }
        
        # åˆ†ç»„æ•°æ®å­˜å‚¨
        self.grouped_data = defaultdict(list)
    
    def _detect_format_and_extract_response(self, record: Dict[str, Any]) -> Tuple[str, str]:
        """æ£€æµ‹æ ¼å¼å¹¶æå–æ¨¡å‹å›å¤å†…å®¹"""
        # ShareGPTæ ¼å¼
        if 'conversations' in record:
            conversations = record['conversations']
            if isinstance(conversations, list) and conversations:
                # æ‰¾åˆ°æœ€åä¸€ä¸ªgpt/assistantå›å¤
                for conv in reversed(conversations):
                    if isinstance(conv, dict) and conv.get('from') in ['gpt', 'assistant']:
                        return 'sharegpt', conv.get('value', '')
            return 'sharegpt', ''
        
        # Instructionæ ¼å¼
        elif 'instruction' in record and 'output' in record:
            return 'instruction', record.get('output', '')
        
        # å…¶ä»–å¯èƒ½çš„æ ¼å¼
        elif 'response' in record:
            return 'unknown', record.get('response', '')
        elif 'answer' in record:
            return 'unknown', record.get('answer', '')
        
        return 'unknown', ''
    
    def _count_characters(self, text: str) -> int:
        """è®¡ç®—æ–‡æœ¬çš„å­—ç¬¦æ•°é‡"""
        return len(text) if text else 0
    
    def load_and_analyze_data(self):
        """åŠ è½½æ•°æ®å¹¶åˆ†æé•¿åº¦åˆ†å¸ƒ"""
        print(f"ğŸ“– å¼€å§‹åŠ è½½å’Œåˆ†ææ–‡ä»¶: {self.input_file}")
        
        with open(self.input_file, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if not line:
                    continue
                
                try:
                    record = json.loads(line)
                    self.stats['total_records'] += 1
                    
                    # æ£€æµ‹æ ¼å¼å¹¶æå–å›å¤å†…å®¹
                    format_type, response_text = self._detect_format_and_extract_response(record)
                    self.stats['format_counts'][format_type] += 1
                    
                    # è®¡ç®—å­—ç¬¦é•¿åº¦
                    char_length = self._count_characters(response_text)
                    
                    # å­˜å‚¨æ•°æ®
                    record['_char_length'] = char_length
                    record['_format_type'] = format_type
                    record['_response_text'] = response_text
                    
                    self.records.append(record)
                    self.lengths.append(char_length)
                    
                    if line_num % 1000 == 0:
                        print(f"ğŸ“Š å·²åŠ è½½ {line_num} æ¡è®°å½•...")
                
                except json.JSONDecodeError as e:
                    print(f"âš ï¸ ç¬¬{line_num}è¡ŒJSONè§£æé”™è¯¯: {e}")
                except Exception as e:
                    print(f"âš ï¸ ç¬¬{line_num}è¡Œå¤„ç†é”™è¯¯: {e}")
        
        # è®¡ç®—é•¿åº¦ç»Ÿè®¡
        if self.lengths:
            self.lengths = np.array(self.lengths)
            self.stats['length_stats'] = {
                'min': int(np.min(self.lengths)),
                'max': int(np.max(self.lengths)),
                'mean': float(np.mean(self.lengths)),
                'std': float(np.std(self.lengths)),
                'median': float(np.median(self.lengths))
            }
        
        print(f"âœ… æ•°æ®åŠ è½½å®Œæˆï¼Œå…± {len(self.records)} æ¡æœ‰æ•ˆè®°å½•")
        print(f"ğŸ“Š é•¿åº¦ç»Ÿè®¡: æœ€å°={self.stats['length_stats']['min']}, æœ€å¤§={self.stats['length_stats']['max']}, å¹³å‡={self.stats['length_stats']['mean']:.1f}")
    
    def perform_clustering(self):
        """æ‰§è¡ŒK-meansèšç±»"""
        if len(self.lengths) == 0:
            print("âŒ æ²¡æœ‰æ•°æ®å¯ä¾›èšç±»")
            return
        
        print(f"ğŸ” å¼€å§‹æ‰§è¡ŒK-meansèšç±»ï¼Œåˆ†ä¸º {self.n_clusters} ç»„...")
        
        # å‡†å¤‡æ•°æ®ï¼ˆå°†é•¿åº¦è½¬æ¢ä¸ºäºŒç»´æ•°ç»„ï¼‰
        X = self.lengths.reshape(-1, 1)
        
        # æ ‡å‡†åŒ–æ•°æ®ï¼ˆå¯é€‰ï¼Œå¯¹äºä¸€ç»´æ•°æ®å½±å“ä¸å¤§ï¼‰
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        # æ‰§è¡ŒK-meansèšç±»
        kmeans = KMeans(n_clusters=self.n_clusters, random_state=42, n_init=10)
        original_labels = kmeans.fit_predict(X_scaled)
        
        # è·å–èšç±»ä¸­å¿ƒï¼ˆè½¬æ¢å›åŸå§‹å°ºåº¦ï¼‰
        cluster_centers = scaler.inverse_transform(kmeans.cluster_centers_).flatten()
        
        # è®¡ç®—æ¯ä¸ªèšç±»çš„å¹³å‡é•¿åº¦å¹¶åˆ›å»ºæ’åºæ˜ å°„
        cluster_mean_lengths = []
        for i in range(self.n_clusters):
            cluster_mask = original_labels == i
            cluster_lengths = self.lengths[cluster_mask]
            mean_length = float(np.mean(cluster_lengths))
            cluster_mean_lengths.append((i, mean_length))
        
        # æŒ‰å¹³å‡é•¿åº¦æ’åºï¼Œè·å–æ–°çš„clusteråºå·æ˜ å°„
        cluster_mean_lengths.sort(key=lambda x: x[1])  # æŒ‰å¹³å‡é•¿åº¦ä»ä½åˆ°é«˜æ’åº
        
        # åˆ›å»ºåŸå§‹cluster_idåˆ°æ–°cluster_idçš„æ˜ å°„
        old_to_new_mapping = {}
        for new_id, (old_id, _) in enumerate(cluster_mean_lengths):
            old_to_new_mapping[old_id] = new_id
        
        # é‡æ–°åˆ†é…clusteræ ‡ç­¾
        self.cluster_labels = np.array([old_to_new_mapping[label] for label in original_labels])
        
        print(f"ğŸ“Š èšç±»é‡æ’åºæ˜ å°„: {old_to_new_mapping}")
        
        # åˆ†ææ¯ä¸ªèšç±»ï¼ˆä½¿ç”¨æ–°çš„æ’åºåçš„IDï¼‰
        for new_id in range(self.n_clusters):
            cluster_mask = self.cluster_labels == new_id
            cluster_lengths = self.lengths[cluster_mask]
            
            cluster_info = {
                'count': int(np.sum(cluster_mask)),
                'min_length': int(np.min(cluster_lengths)),
                'max_length': int(np.max(cluster_lengths)),
                'mean_length': float(np.mean(cluster_lengths)),
                'median_length': float(np.median(cluster_lengths)),
                'center': float(cluster_centers[old_to_new_mapping.get(new_id, new_id)])
            }
            
            self.stats['cluster_info'][f'cluster_{new_id}'] = cluster_info
            
            print(f"ğŸ“Š èšç±» {new_id}: {cluster_info['count']} æ¡è®°å½•, é•¿åº¦èŒƒå›´ [{cluster_info['min_length']}-{cluster_info['max_length']}], å¹³å‡ {cluster_info['mean_length']:.1f}")
        
        print("âœ… èšç±»å®Œæˆï¼ˆå·²æŒ‰é•¿åº¦æ’åºï¼‰")
    
    def group_data_by_clusters(self):
        """æ ¹æ®èšç±»ç»“æœåˆ†ç»„æ•°æ®"""
        print("ğŸ“ æ ¹æ®èšç±»ç»“æœåˆ†ç»„æ•°æ®...")
        
        for i, record in enumerate(self.records):
            cluster_id = self.cluster_labels[i]
            cluster_name = f"cluster_{cluster_id}"
            
            # æ·»åŠ èšç±»ä¿¡æ¯åˆ°è®°å½•
            record['_cluster_id'] = int(cluster_id)
            record['_cluster_name'] = cluster_name
            
            # åˆ†ç»„å­˜å‚¨
            self.grouped_data[cluster_name].append(record)
        
        print("âœ… æ•°æ®åˆ†ç»„å®Œæˆ")
    
    def save_grouped_data(self):
        """ä¿å­˜åˆ†ç»„æ•°æ®åˆ°ä¸åŒæ–‡ä»¶å¤¹"""
        print(f"ğŸ’¾ å¼€å§‹ä¿å­˜åˆ†ç»„æ•°æ®åˆ°: {self.output_dir}")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        for cluster_name, records in self.grouped_data.items():
            if not records:
                continue
            
            # åˆ›å»ºèšç±»æ–‡ä»¶å¤¹
            cluster_dir = self.output_dir / cluster_name
            cluster_dir.mkdir(exist_ok=True)
            
            # ä¿å­˜æ•°æ®
            output_file = cluster_dir / f"{cluster_name}_data.jsonl"
            with open(output_file, 'w', encoding='utf-8') as f:
                for record in records:
                    # ç§»é™¤ä¸´æ—¶æ·»åŠ çš„å­—æ®µï¼ˆå¯é€‰ï¼‰
                    clean_record = {k: v for k, v in record.items() 
                                  if not k.startswith('_')}
                    f.write(json.dumps(clean_record, ensure_ascii=False) + '\n')
            
            cluster_info = self.stats['cluster_info'][cluster_name]
            print(f"ğŸ“ {cluster_name}: {len(records)} æ¡è®°å½• (é•¿åº¦èŒƒå›´: {cluster_info['min_length']}-{cluster_info['max_length']}) -> {output_file}")
    
    def save_statistics(self):
        """ä¿å­˜ç»Ÿè®¡ä¿¡æ¯"""
        stats_file = self.output_dir / "clustering_statistics.json"
        
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(self.stats, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜åˆ°: {stats_file}")
    
    def plot_length_distribution(self):
        """ç»˜åˆ¶é•¿åº¦åˆ†å¸ƒå’Œèšç±»ç»“æœå›¾"""
        try:
            plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
            plt.rcParams['axes.unicode_minus'] = False
            
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
            
            # é•¿åº¦åˆ†å¸ƒç›´æ–¹å›¾
            ax1.hist(self.lengths, bins=50, alpha=0.7, color='skyblue', edgecolor='black')
            ax1.set_xlabel('Response Length (Characters)')
            ax1.set_ylabel('Frequency')
            ax1.set_title('Model Response Length Distribution')
            ax1.grid(True, alpha=0.3)
            
            # èšç±»ç»“æœæ•£ç‚¹å›¾
            colors = plt.cm.Set3(np.linspace(0, 1, self.n_clusters))
            for i in range(self.n_clusters):
                cluster_mask = self.cluster_labels == i
                cluster_lengths = self.lengths[cluster_mask]
                y_positions = np.random.normal(i, 0.1, len(cluster_lengths))
                
                ax2.scatter(cluster_lengths, y_positions, 
                           c=[colors[i]], alpha=0.6, s=20, 
                           label=f'Cluster {i} ({np.sum(cluster_mask)} items)')
            
            ax2.set_xlabel('Response Length (Characters)')
            ax2.set_ylabel('Cluster Group')
            ax2.set_title('Clustering Results Distribution')
            ax2.legend()
            ax2.grid(True, alpha=0.3)
            
            plt.tight_layout()
            
            # ä¿å­˜å›¾ç‰‡
            plot_file = self.output_dir / "length_distribution_and_clustering.png"
            plt.savefig(plot_file, dpi=300, bbox_inches='tight')
            print(f"ğŸ“ˆ åˆ†å¸ƒå›¾å·²ä¿å­˜åˆ°: {plot_file}")
            
            plt.close()
            
        except Exception as e:
            print(f"âš ï¸ ç»˜å›¾å¤±è´¥: {e}")
    
    def print_summary(self):
        """æ‰“å°å¤„ç†æ‘˜è¦"""
        print("\n" + "="*60)
        print("ğŸ“Š èšç±»åˆ†ææ‘˜è¦")
        print("="*60)
        print(f"æ€»è®°å½•æ•°: {self.stats['total_records']}")
        print(f"èšç±»æ•°é‡: {self.n_clusters}")
        
        print(f"\næ ¼å¼åˆ†å¸ƒ:")
        for fmt, count in self.stats['format_counts'].items():
            if count > 0:
                percentage = (count / self.stats['total_records']) * 100
                print(f"  - {fmt}: {count} ({percentage:.1f}%)")
        
        print(f"\næ•´ä½“é•¿åº¦ç»Ÿè®¡:")
        stats = self.stats['length_stats']
        print(f"  - æœ€å°é•¿åº¦: {stats['min']} å­—ç¬¦")
        print(f"  - æœ€å¤§é•¿åº¦: {stats['max']} å­—ç¬¦")
        print(f"  - å¹³å‡é•¿åº¦: {stats['mean']:.1f} å­—ç¬¦")
        print(f"  - ä¸­ä½æ•°é•¿åº¦: {stats['median']:.1f} å­—ç¬¦")
        print(f"  - æ ‡å‡†å·®: {stats['std']:.1f}")
        
        print(f"\nå„èšç±»è¯¦æƒ…:")
        for cluster_name, info in self.stats['cluster_info'].items():
            print(f"  - {cluster_name}: {info['count']} æ¡è®°å½•")
            print(f"    é•¿åº¦èŒƒå›´: [{info['min_length']}-{info['max_length']}] å­—ç¬¦")
            print(f"    å¹³å‡é•¿åº¦: {info['mean_length']:.1f} å­—ç¬¦")
            print(f"    ä¸­ä½æ•°é•¿åº¦: {info['median_length']:.1f} å­—ç¬¦")
        
        print("="*60)
    
    def run(self):
        """è¿è¡Œå®Œæ•´æµç¨‹"""
        self.load_and_analyze_data()
        self.perform_clustering()
        self.group_data_by_clusters()
        self.save_grouped_data()
        self.save_statistics()
        self.plot_length_distribution()
        self.print_summary()


def main():
    parser = argparse.ArgumentParser(
        description="JSONLæ–‡ä»¶èšç±»åˆ†å‰²å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  python JsonlClusterSplitter.py input.jsonl output_dir
  python JsonlClusterSplitter.py input.jsonl output_dir --clusters 3
  python JsonlClusterSplitter.py input.jsonl output_dir --clusters 8
        """
    )
    
    parser.add_argument('--input_file', default= "./data/open-r1/Mixture-of-Thoughts/mix_train_data/instruct_processed/train/science/science_merged.jsonl", help='è¾“å…¥çš„JSONLæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output_dir', default= "./data/open-r1/Mixture-of-Thoughts/mix_train_data/instruct_processed/train/science", help='è¾“å‡ºç›®å½•è·¯å¾„')
    parser.add_argument('--clusters', '-c', type=int, default=5, 
                       help='èšç±»æ•°é‡ï¼ˆé»˜è®¤ï¼š5ï¼‰')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(args.input_file):
        print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input_file}")
        return
    
    # æ£€æŸ¥èšç±»æ•°é‡
    if args.clusters < 2:
        print(f"âŒ èšç±»æ•°é‡å¿…é¡»å¤§äºç­‰äº2")
        return
    
    # åˆ›å»ºåˆ†å‰²å™¨
    splitter = JsonlClusterSplitter(
        input_file=args.input_file,
        output_dir=args.output_dir,
        n_clusters=args.clusters
    )
    
    # è¿è¡Œå¤„ç†
    try:
        splitter.run()
        print(f"\nğŸ‰ èšç±»åˆ†å‰²å®Œæˆï¼ç»“æœä¿å­˜åœ¨: {args.output_dir}")
    except Exception as e:
        print(f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()