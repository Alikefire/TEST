full_data_path: ./data/open-r1/Mixture-of-Thoughts/mix_train_data/instruct_108k_processed/train/code/code_merged_36k.jsonl, ./data/open-r1/Mixture-of-Thoughts/mix_train_data/instruct_108k_processed/train/math/math_merged_36k.jsonl, ./data/open-r1/Mixture-of-Thoughts/mix_train_data/instruct_108k_processed/train/science/science_merged_36k.jsonl
model_name_or_path: ./Model/OriginalModel/Qwen/Qwen3-0.6B   # path to pretrained foundation llm
cache_dir: data/huggingface_models
model_max_length: 32768
schedule_name: Full
result_dir_name: ./Model/MergeModel/Qwen3-0.6B-Mot_mixed-sft-194.4k #checkpoint所在目录
train_args:
  optim: adamw_torch
  num_train_epochs: 1
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  gradient_accumulation_steps: 3
  eval_strategy: "no"
  save_strategy: "steps"
  save_steps: 300
  save_total_limit: 20
  learning_rate: 1.0e-5
  weight_decay: 0.
  warmup_ratio: 0.03
  lr_scheduler_type: cosine
  logging_steps: 10
  fsdp: "full_shard auto_wrap" 
  # fp16: true
  bf16: true
  full_determinism: TRUE
  seed: 42
