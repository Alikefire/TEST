full_data_path: ./data/open-r1/Mixture-of-Thoughts/mix_train_data/instruct_4k_processed/train/code/all_domain_merged.jsonl, ./data/open-r1/Mixture-of-Thoughts/mix_train_data/instruct_4k_processed/train/math/all_domain_merged.jsonl, ./data/open-r1/Mixture-of-Thoughts/mix_train_data/instruct_4k_processed/train/science/all_domain_merged.jsonl
model_name_or_path: ./Model/OriginalModel/Qwen/Qwen2.5-0.5B-Instruct   # path to pretrained foundation llm
cache_dir: data/huggingface_models
model_max_length: 32768
schedule_name: Full
result_dir_name: ./Model/MergeModel/Qwen2.5-0.5B-Instruc-Mot_mix-sft-4k #checkpoint所在目录
train_args:
  optim: adamw_torch
  num_train_epochs: 1
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  gradient_accumulation_steps: 16
  eval_strategy: "no"
  save_strategy: "steps"
  save_steps: 200
  save_total_limit: 20
  learning_rate: 1.0e-5
  weight_decay: 0.
  warmup_ratio: 0.03
  lr_scheduler_type: cosine
  logging_steps: 10
  fsdp: "full_shard auto_wrap" 
  fp16: true
  full_determinism: TRUE
  seed: 42
