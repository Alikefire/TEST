full_data_path: ./data/simplescaling/s1K-1.1/data/train.json, ./data/VLyb/s1K-mix/s1_brief_cot.json, ./data/N8Programs/gsm8k-r1/train.jsonl, ./data/N8Programs/gsm8k-gpt4o/train.json
model_name_or_path: ./Model/OriginalModel/Qwen/Qwen2.5-0.5B-Instruct  # path to pretrained foundation llm
cache_dir: data/huggingface_models
model_max_length: 25000
schedule_name: Full
result_dir_name: ./Model/MergeModel/DeepSeek-R1-Distill-Qwen-0.5B-long_short-sft-more_ckpt
train_args:
  optim: adamw_torch
  num_train_epochs: 3
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  gradient_accumulation_steps: 16
  eval_strategy: "no"
  save_strategy: "steps"
  save_steps: 200
  save_total_limit: 20
  learning_rate: 1.0e-5
  weight_decay: 0.
  warmup_ratio: 0.03
  lr_scheduler_type: cosine
  logging_steps: 1
  fsdp: "full_shard auto_wrap" 
  fp16: true
  full_determinism: TRUE
  seed: 42
